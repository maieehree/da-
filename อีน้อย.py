# Updated version for Google Colab
from flask import Flask, request, jsonify, render_template_string
import torch
import os
import gc
import time
from transformers import AutoTokenizer, AutoModelForCausalLM
from google.colab import output, drive

# 1. Mount Google Drive ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•
if not os.path.exists('/content/drive'):
    drive.mount('/content/drive')

app = Flask(__name__)

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Path ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ô Google Drive
MODEL_PATH = '/content/drive/MyDrive/fine_tuned_tinyllama.pth'

model = None
tokenizer = None
last_activity_time = time.time()

def ensure_folders():
    if not os.path.exists("offload"):
        os.makedirs("offload")

def load_model():
    print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• TinyLLaMA...")
    try:
        global model, tokenizer
        ensure_folders()

        tokenizer = AutoTokenizer.from_pretrained(
            "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            trust_remote_code=True
        )

        model = AutoModelForCausalLM.from_pretrained(
            "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="auto",
            low_cpu_mem_usage=True,
            offload_folder="offload"
        )

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå fine-tuned weights ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if os.path.exists(MODEL_PATH):
            print(f"‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Fine-tuned ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å: {MODEL_PATH}")
            state_dict = torch.load(MODEL_PATH, map_location='cpu')
            model.load_state_dict(state_dict, strict=False)
            del state_dict
        else:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå fine-tuned ‡πÉ‡∏ô Drive ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Base Model ‡πÅ‡∏ó‡∏ô")

        model.eval()
        torch.cuda.empty_cache()
        gc.collect()
        print("‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        return True
    except Exception as e:
        print(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
        return False

def generate_response(user_input):
    global model, tokenizer
    try:
        # ‡∏õ‡∏£‡∏±‡∏ö Prompt ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö TinyLlama Chat Format
        prompt = f"<|user|>\n{user_input}\n<|assistant|>\n"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # ‡∏ï‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á AI
        if "<|assistant|>" in full_response:
            response = full_response.split("<|assistant|>")[-1].strip()
        else:
            response = full_response.replace(user_input, "").strip()
        
        return response
    except Exception as e:
        return f"Error: {str(e)}"

# --- UI HTML ---
CHAT_HTML = """
<!DOCTYPE html>
<html lang="th">
<head>
    <meta charset="UTF-8">
    <title>YaDa AI Chatbot</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body { margin: 0; font-family: sans-serif; background: #121212; color: white; display: flex; justify-content: center; height: 100vh; }
        .chat-container { width: 100%; max-width: 600px; display: flex; flex-direction: column; background: #1e1e1e; }
        .chat-header { padding: 20px; background: #7928ca; text-align: center; font-weight: bold; }
        .chat-box { flex: 1; padding: 20px; overflow-y: auto; display: flex; flex-direction: column; gap: 10px; }
        .msg { padding: 10px 15px; border-radius: 15px; max-width: 80%; }
        .user { background: #0072ff; align-self: flex-end; }
        .bot { background: #333; align-self: flex-start; }
        .input-area { padding: 20px; display: flex; border-top: 1px solid #333; }
        input { flex: 1; padding: 10px; border-radius: 5px; border: none; outline: none; }
        button { margin-left: 10px; padding: 10px 20px; background: #7928ca; color: white; border: none; border-radius: 5px; cursor: pointer; }
    </style>
</head>
<body>
    <div class="chat-container">
        <div class="chat-header">üí¨ YaDa AI Chatbot</div>
        <div class="chat-box" id="chatBox">
            <div class="msg bot">‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ! ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞?</div>
        </div>
        <div class="input-area">
            <input type="text" id="userInput" placeholder="‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...">
            <button onclick="sendMessage()">‡∏™‡πà‡∏á</button>
        </div>
    </div>
    <script>
        async function sendMessage() {
            const input = document.getElementById('userInput');
            const text = input.value.trim();
            if(!text) return;
            
            const chatBox = document.getElementById('chatBox');
            chatBox.innerHTML += `<div class="msg user">${text}</div>`;
            input.value = '';

            const res = await fetch('/api/chat', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ input: text })
            });
            const data = await res.json();
            chatBox.innerHTML += `<div class="msg bot">${data.response}</div>`;
            chatBox.scrollTop = chatBox.scrollHeight;
        }
    </script>
</body>
</html>
"""

@app.route('/')
def home():
    return render_template_string(CHAT_HTML)

@app.route('/api/chat', methods=['POST'])
def chat():
    if model is None:
        if not load_model():
            return jsonify({'response': '‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à'})
    
    data = request.json
    user_input = data.get('input', '')
    response = generate_response(user_input)
    return jsonify({'response': response})

if __name__ == '__main__':
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Port
    PORT = 5000
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á URL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÉ‡∏ô Colab
    output.serve_kernel_port_as_window(PORT)
    print(f"‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö (‡∏£‡∏≠‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ Flask ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)")
    app.run(port=PORT)
